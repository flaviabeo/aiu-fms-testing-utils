System:
Solve the following coding problem. Wrap your code answer using ```


Question:
Implement batch gradient descent using JAX's automatic differentiation (`jax.grad`) to optimize a linear regression model. Compare the model's performance with stochastic gradient descent (SGD).


Answer:
```python
import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate a synthetic dataset
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the linear regression model
def linear_regression(params, X):
    w, b = params
    return jnp.dot(X, w) + b

# Define the loss function
def loss(params, X, y):
    y_pred = linear_regression(params, X)
    return jnp.mean((y_pred - y) ** 2)

# Define the gradient of the loss function
grad_loss = grad(loss)

# Initialize the model parameters
w_init = jnp.zeros(X.shape[1])
b_init = jnp.zeros(1)
params_init = (w_init, b_init)

# Define the batch gradient descent update rule
def batch_gradient_descent(params, X, y, learning_rate, num_epochs):
    for _ in range(num_epochs):
        grads = grad_loss(params, X, y)
        params = (params[0] - learning_rate * grads[0], params[1] - learning_rate * grads[1])
    return params

# Define the stochastic gradient descent update rule
def stochastic_gradient_descent(params, X, y, learning_rate, batch_size):
    indices = jnp.arange(X.shape[0])
    for _ in range(num_epochs):
        jnp.random.shuffle(indices)
        for i in range(0, X.shape[0], batch_size):
            X_batch = X[indices[i:i + batch_size]]
            y_batch = y[indices[i:i + batch_size]]